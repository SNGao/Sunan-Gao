[
  {
    "objectID": "example_analysis.html",
    "href": "example_analysis.html",
    "title": "Example analysis",
    "section": "",
    "text": "Background\nThere are two commonly held beliefs around income and age (Routley 2018):  - Earning trajectory is largely determined by the time a person is 35-years-old  - Income is positively correlated with age \nFigure: Visualize American Income Levels by Age Group (Routley 2018)  \n\n\nMain Problem\n\nIs it possible that the relationship between age and income is non-linear?\nCould we use some models to illustrate such a relationship?\n\n\n\nIntended Audience\n\nDemographers interested in the correlation between age and income \nStudents who are studying statistical models\n\n\n\nDataset\nData was manually assembled by Steve Miller, of Inquidia Consulting. From the Mar. 2011 Supplement to Current Population Survey data. Wage and other data for a group of 3000 male workers in the Mid-Atlantic region.  - Description  - Source Resources\n\n\nData dictionary\nData Dictionary\n\n\nDescriptive Analysis\n\nThe data of 5 years before the 2008 financial crisis (2004-2008) were selected as the target.\nWe mainly foceus on the associations between wage and age. Thus, most variables were dropped in the following analysis.\n\n\ndata = Wage; Wage = data\nWage &lt;- Wage %&gt;%\n          filter(year&gt;=2004 & year&lt;=2008) %&gt;%\n          dplyr::select(year, age, wage, jobclass, education)\nhead(Wage)\n\n  year age      wage       jobclass       education\n1 2006  18  75.04315  1. Industrial    1. &lt; HS Grad\n2 2004  24  70.47602 2. Information 4. College Grad\n3 2005  50  75.04315 2. Information      2. HS Grad\n4 2008  54 127.11574 2. Information 4. College Grad\n5 2008  30 111.72085 2. Information 3. Some College\n6 2006  41 118.88436 2. Information 3. Some College\n\n\nUnder the stratification of education and jobclass, the average and variance of wage.\n\nWage %&gt;% \n  group_by(education, jobclass) %&gt;%\n  summarise(Mean = mean(wage),\n            std = sd(wage))\n\n`summarise()` has grouped output by 'education'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 10 × 4\n# Groups:   education [5]\n   education          jobclass        Mean   std\n   &lt;fct&gt;              &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt;\n 1 1. &lt; HS Grad       1. Industrial   84.4  20.8\n 2 1. &lt; HS Grad       2. Information  88.4  22.4\n 3 2. HS Grad         1. Industrial   94.7  27.1\n 4 2. HS Grad         2. Information  97.4  26.7\n 5 3. Some College    1. Industrial  108.   27.8\n 6 3. Some College    2. Information 109.   32.0\n 7 4. College Grad    1. Industrial  120.   39.4\n 8 4. College Grad    2. Information 127.   41.9\n 9 5. Advanced Degree 1. Industrial  143.   51.2\n10 5. Advanced Degree 2. Information 157.   56.9\n\n\n\nPeople with higher levels of education and working in the information field generally have higher incomes, but they fluctuate more\n\n\nWage %&gt;% \n  group_by(age) %&gt;%\n  summarise(Mean = mean(wage),\n            std = sd(wage)) %&gt;%\n  ggplot(aes(x = age, y = Mean)) + \n  geom_line(aes(x = age, y = Mean),  linetype = \"solid\", color = rainbow(61), size = 1.5) + \n  labs(title = \"The average wage under different age\",\n       caption = \"Data source: Wage data was manually assembled by Steve Miller\",\n       x = \"Age\", y = \"Wage\") + theme_minimal()\n\n\n\n\n\nWage %&gt;% \n  group_by(education) %&gt;% \n  ggplot(aes(x = age, y = wage)) +\n  geom_bar(stat = \"identity\", fill = rainbow(2098)) +\n  labs(title = \"The distribution of wage under different age\",\n       caption = \"Data source: Wage data was manually assembled by Steve Miller\",\n       x = \"Age\", y = \"Wage\") +\n  theme_minimal()\n\n\n\n\n\nWage rises gradually between the ages of 20 and 40, gradually stabilizes after the age of 40, and gradually declines after the age of 60. There is a clear rise around age 75, probably because the data sample is smaller. On the whole, age and wage do not show a linear increase relationship, but a non-linear increase.\n\n\nWage %&gt;% \n  group_by(jobclass, education) %&gt;%\n  mutate(Mean_diff = wage - mean(wage),\n         std = sd(wage)) %&gt;%\n  ggplot(aes(x = age, y = Mean_diff)) + \n  geom_bar(stat = \"identity\", fill = rainbow(2098)) + \n  labs(title = \"The distribution of the income difference relative to the same jobclass/education group at different ages\",\n       caption = \"Data source: Wage data was manually assembled by Steve Miller\",\n       x = \"Age\", y = \"Wage\") +\n  theme_minimal()\n\n\n\n\n\nWhen stratified by jobclass and education level, it can be noted that the distribution trend is not the same for different ages. At the same time, the distribution characteristics of wage difference and individual wage also change significantly after group calculation, which provides ideas for subsequent variable adjustment and subgroup analysis.\n\n\nfit &lt;- lm(wage~poly(age,4),data=Wage)\nsummary(fit)\n\n\nCall:\nlm(formula = wage ~ poly(age, 4), data = Wage)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-99.520 -23.912  -5.245  15.092 203.535 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    112.2584     0.8642 129.893   &lt;2e-16 ***\npoly(age, 4)1  406.9399    39.5853  10.280   &lt;2e-16 ***\npoly(age, 4)2 -396.0329    39.5853 -10.005   &lt;2e-16 ***\npoly(age, 4)3   57.3070    39.5853   1.448    0.148    \npoly(age, 4)4  -33.7026    39.5853  -0.851    0.395    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.59 on 2093 degrees of freedom\nMultiple R-squared:  0.09063,   Adjusted R-squared:  0.08889 \nF-statistic: 52.15 on 4 and 2093 DF,  p-value: &lt; 2.2e-16\n\n\n\nIt can be observed in the polynomial model that the significance of the quadratic age provides the basis for the nonlinear correlation.\n\n\nagelims &lt;- range(Wage$age)\nage.grid &lt;- seq(from=agelims[1], to=agelims[2])\nnewdata &lt;- list(age=age.grid)\npreds &lt;- predict(fit, newdata, se=TRUE)\n\nse.bands &lt;- cbind(preds$fit+2*preds$se, preds$fit-2*preds$se)\ndata_plot = data.frame(newdata, preds$fit, se.bands); colnames(data_plot)=c('Age','fit','Upper','Lower')\n\nggplot(data = data_plot, aes(x = Age, y = fit)) +\n  geom_point(data = Wage, aes(x = age, y = wage), color = \"darkgrey\") +\n  geom_ribbon(data = data_plot, aes(x = Age, ymin = Lower, ymax = Upper), color = \"blue\", alpha = 0.3) +\n  geom_line(data = data_plot, aes(x = Age, y = fit), linetype = \"solid\", color = \"blue\", size = 1.5) +\n  facet_wrap(~jobclass, ncol = NULL, scales = \"free_y\") +\n  labs(title = \"The non linear connection between Age and Wage\",\n       subtitle = 'Polynomials Regression, max power = 4',\n       caption = \"Data source: Wage data was manually assembled by Steve Miller\",\n       x = \"Age\", y = \"Wage\") +\n  theme_minimal()\n\n\n\n\n\nThe correlation between Wage and age is basically the same in different jobclasses, which is the same as previously discussed. Interestingly, the highest peaks are reached in middle age, covering the ages of 35-65.\n\nWe use the anova() function, which performs an analysis of variance (ANOVA, using an F-test) in order to test the null hypothesis that a model M1 is sufficient to explain the data against the alternative hypothesis that a more complex model M2 is required.\n\nfit.1 &lt;- lm(wage ~ age,data=Wage)\nfit.2 &lt;- lm(wage ~ poly(age,2),data=Wage)\nfit.3 &lt;- lm(wage ~ poly(age,3),data=Wage)\nfit.4 &lt;- lm(wage ~ poly(age,4),data=Wage) \nfit.5 &lt;- lm(wage ~ poly(age,5),data=Wage)\nanova(fit.1,fit.2,fit.3,fit.4,fit.5)\n\nAnalysis of Variance Table\n\nModel 1: wage ~ age\nModel 2: wage ~ poly(age, 2)\nModel 3: wage ~ poly(age, 3)\nModel 4: wage ~ poly(age, 4)\nModel 5: wage ~ poly(age, 5)\n  Res.Df     RSS Df Sum of Sq        F Pr(&gt;F)    \n1   2096 3440989                                 \n2   2095 3284147  1    156842 100.1179 &lt;2e-16 ***\n3   2094 3280863  1      3284   2.0964 0.1478    \n4   2093 3279727  1      1136   0.7251 0.3946    \n5   2092 3277272  1      2455   1.5671 0.2108    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nANOVA results show that only the square of Age can increase the explanatory power of Wage in the model, and there is no higher-order relationship between them.\n\n\n\nPolynomial logistic regression\nNow we fit a logistic regression model to a binary response variable, constructed from wage. We code the big earners (&gt;250K) as 1, else 0.\n\nfit &lt;- glm(I(wage&gt;250) ~ poly(age,3), data=Wage, family=binomial)\nsummary(fit)\n\n\nCall:\nglm(formula = I(wage &gt; 250) ~ poly(age, 3), family = binomial, \n    data = Wage)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.3184  -0.2872  -0.2484  -0.1440   3.1003  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -4.0998     0.2712 -15.116  &lt; 2e-16 ***\npoly(age, 3)1  52.2096    17.0657   3.059  0.00222 ** \npoly(age, 3)2 -40.6636    14.8921  -2.731  0.00632 ** \npoly(age, 3)3  15.4169    10.5255   1.465  0.14300    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 516.31  on 2097  degrees of freedom\nResidual deviance: 490.97  on 2094  degrees of freedom\nAIC: 498.97\n\nNumber of Fisher Scoring iterations: 8\n\npreds &lt;- predict(fit,list(age=age.grid),se=T)\nse.bands &lt;- preds$fit + cbind(fit=0,lower=-2*preds$se,upper=2*preds$se)\nse.bands[1:5,]\n\n         fit     lower     upper\n1 -10.693521 -16.22091 -5.166129\n2 -10.085397 -15.08159 -5.089209\n3  -9.509273 -14.00919 -5.009360\n4  -8.964341 -13.00187 -4.926814\n5  -8.449794 -12.05780 -4.841791\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere I() is a wrapper function; A wrapper function is a function in a software library or a computer program whose main purpose is to call a second subroutine or a system call with little or no additional computation “Wrapper Function” (2023).\n\n\n\n\nThe computations are on the logit scale. To transform, we need to apply the inverse logit mapping \\[p=\\frac{e^\\eta}{1+e^\\eta}.\\]\nIn order to perform local regression, we use the loess() function.\n\nggplot(data = Wage, aes(x = age, y = wage)) +\n  geom_point(color = \"darkgrey\", size = 2) +\n  geom_smooth(method = \"loess\", formula = y ~ x, span = 0.2, color = \"red\", size = 1) +\n  geom_smooth(method = \"loess\", formula = y ~ x, span = 0.5, color = \"blue\", size = 1) +\n  xlim(agelims) +\n  labs(title = \"Local Regression\", caption = \"Span=0.2 (red), Span=0.5 (blue)\") +\n  theme_minimal() +\n  theme(legend.position = \"topright\") +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  guides(color = guide_legend(title = NULL))\n\n\n\n\n\nLocal regression describes the nonlinear joint with a higher degree of freedom model(Cleveland and Loader 1996), and the results are in good agreement with the multi-variable model.\n\nTo compare the figure between plot function in base package and ggplot function, I show you the figure without using ggplot.\n\n\n\n\n\n\n\n\n\n\n\nAdditional information for this analysis\n\n\n\n\n\nThis study focuses on the correlation between Wage and Age, but does not discuss the stratification and adjustment of variables. Meanwhile, the predicted value of the model comes from the training sample of the model, which will interfere with the actual fitting effect. More rigorous additional analysis is needed.\n\n\n\n\n\nSummary of Analysis\n\nPeople with higher levels of education and working in the information field generally have higher incomes, but they fluctuate more\nWhen stratified by jobclass and education level, it can be noted that the distribution trend is not the same for different ages. At the same time, the distribution characteristics of wage difference and individual wage also change significantly after group calculation, which provides ideas for subsequent variable adjustment and subgroup analysis.\nANOVA results show that only the square of Age can increase the explanatory power of Wage in the model, and there is no higher-order relationship between them.\nLocal regression describes the nonlinear joint with a higher degree of freedom model, and the results are in good agreement with the multi-variable model.\n\n\n\nFunctions used from packages\n\ndplyr/tidyr: filter(); select(); summarise(); group_by(); mutate() \nggplot2: geom_point(); geom_smooth(); geom_line(); geom_ribbon(); geom_bar\n\n\n\n\n\n\nReferences\n\nCleveland, William S., and Clive Loader. 1996. “Smoothing by Local Regression: Principles and Methods.” In Statistical Theory and Computational Aspects of Smoothing, edited by Wolfgang Härdle and Michael G. Schimek, 10–49. Contributions to Statistics. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-48425-4_2.\n\n\nJedermann, Reiner, and Walter Lang. 2022. “Wrapper Functions for Integrating Mathematical Models into Digital Twin Event Processing.” Sensors 22 (20): 7964. https://doi.org/10.3390/s22207964.\n\n\nRoutley, Nick. 2018. “Visualizing American Income Levels by Age Group. Visual Capitalist.” December 5, 2018. https://www.visualcapitalist.com/american-income-levels-by-age-group/.\n\n\n“Wrapper Function.” 2023. In Wikipedia. https://en.wikipedia.org/w/index.php?title=Wrapper_function&oldid=1176415388#cite_note-1."
  }
]